---
title: "Portfolio 2"
---

### Goal

In this project, I revisit my completed experiment on AI question framing and emotional distress in college upperclassmen and translate the SPSS dataset into tidy R tibbles that I can use for further exploratory analysis. The original study recruited 60 U.S. juniors and seniors to report their post‑graduation plan clarity, respond to either two specific or two open‑ended AI‑generated purpose questions, and then rate their negative affect, with the main finding that higher plan clarity predicted lower distress while AI question type and its interaction with clarity were non‑significant. My goal here is to organize those data in R, creating a clean participant‑level dataset and several grouped datasets (by question type and by clarity levels) that will serve as building blocks for later visualizations and models in my portfolio.

### Products

The product of this project is a small data‑processing pipeline in R that starts from the original SPSS file and produces multiple tidy datasets. First, I use the haven package to import my SPSS file and convert it into a tibble, then I construct a core dataset with one row per participant containing their condition (specific vs open‑ended questions), post‑graduation plan clarity score, and negative affect score. From this core tibble, I create additional derived datasets: one summarizing means and standard deviations by AI question type, and another summarizing negative affect by clarity group (lower vs higher clarity) crossed with question type, which will be used in later portfolio pieces for plotting main effects and potential interactions.


```{r - loading data}
library(tidyverse)
library(haven)

studydata_raw <- read_sav("AI Purpose Questions and Plan Clarity Study.sav") %>% 
  as_tibble()
```

```{r - create dataframe to measure emotional distress scores}
studydata_emotion <- studydata_raw %>% 
  transmute(
    id           = ID,
    condition    = condition,         
    neg_affect   = NegAff_Mean
  )
```

```{r - sep into conditions}
cond1_distress <- studydata_emotion %>% 
  filter(condition == 1) %>% 
  select(id, neg_affect)

cond2_distress <- studydata_emotion %>% 
  filter(condition == 2) %>% 
  select(id, neg_affect)
```

```{r - density plot for neg affect by condition}
ggplot(studydata_emotion,
       aes(x = neg_affect, fill = factor(condition))) +
  geom_density(alpha = 0.4) +
  labs(
    title = "Distribution of Negative Affect by Condition",
    x = "Negative affect (1–5)",
    fill = "Condition"
  ) +
  theme_minimal()
```

Interpretation: Most students report relatively low negative affect (around 1–2 on the 1–5 scale), with fewer students at higher distress levels. There is heavy overlap between the two density curves, with only very small differences across the range of scores, suggesting that the framing of the AI questions did not meaningfully change the distribution of emotional distress in this sample. That is consistent with the arlier result that condition did not significantly predict negative affect.


```{r - create dataframe to measure plan clarity scores}
studydata_clarity <- studydata_raw %>% 
  transmute(
    id           = ID,
    condition    = condition,
    plan_clarity = PlanClarity_Mean
  )
```

```{r sep by conditions}
cond1_clarity <- studydata_clarity %>%
  filter(condition == 1) %>%
  select(id, plan_clarity)

cond2_clarity <- studydata_clarity %>%
  filter(condition == 2) %>%
  select(id, plan_clarity)
```

```{r - density plot for plan clarity scores by condition}
ggplot(studydata_clarity,
       aes(x = plan_clarity, fill = factor(condition))) +
  geom_density(alpha = 0.4) +
  labs(
    title = "Distribution of Plan Clarity by Condition",
    x = "Post-graduation plan clarity (1–5)",
    fill = "Condition"
  ) +
  theme_minimal()
```

Interpretation: The plot shows that plan‑clarity scores in the two AI question conditions have very similar distributions, with most students falling in the mid‑range with no clear separation between conditions 1 and 2. This suggests that random assignment to either condition of specific versus open‑ended AI questions did not meaningfully change how clear students felt about their post‑graduation plans, which fits the design where clarity was measured as a baseline individual difference rather than as an outcome of the manipulation.


```{r - create clarity groups for distress summary}
median_clarity <- median(studydata_raw$PlanClarity_Mean, na.rm = TRUE)

studydata_distresssummary <- studydata_raw %>%
  transmute(
    id           = ID,
    condition    = condition,
    plan_clarity = PlanClarity_Mean,
    neg_affect   = NegAff_Mean
  ) %>%
  mutate(
    clarity_group = if_else(plan_clarity < median_clarity,
                            "Lower clarity", "Higher clarity")
  )
```
```{r - neg affect summary by clarity group and condition}
clarity_summary <- studydata_distresssummary %>%
  group_by(clarity_group, condition) %>%
  summarize(
    n           = n(),
    mean_affect = mean(neg_affect, na.rm = TRUE),
    sd_affect   = sd(neg_affect, na.rm = TRUE),
    .groups = "drop"
  )
```


